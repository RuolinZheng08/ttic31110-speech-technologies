{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import editdistance\n",
    "import os\n",
    "np.seterr(divide='ignore') # masks log(0) errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.loader import make_loader, Preprocessor\n",
    "from rnn.model import Seq2Seq\n",
    "from rnn.model import LinearND #Hint: this is useful when defining the modified attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default config, you can get 10-15% dev WER within 20 epochs. Each epoch will take 5 secs on a GPU and 10 secs on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim, attn_dim=None):\n",
    "        \"\"\"\n",
    "        Initialize Attention.\n",
    "        ----\n",
    "        enc_dim: encoder hidden state dimension\n",
    "        dec_dim: decoder hidden state dimension\n",
    "        attn_dim: attention feature dimension\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        if enc_dim == dec_dim and attn_dim is None:\n",
    "            self.use_default = True\n",
    "        elif attn_dim is not None:\n",
    "            self.use_default = False\n",
    "            self.attn_dim = attn_dim\n",
    "            self.enc_dim = enc_dim\n",
    "            self.dec_dim = dec_dim\n",
    "            self.v = LinearND(self.attn_dim, 1, bias=False)\n",
    "            self.W1 = LinearND(self.enc_dim, self.attn_dim, bias=False)\n",
    "            self.W2 = nn.Linear(self.dec_dim, self.attn_dim, bias=False)\n",
    "        else:\n",
    "            raise ValueError(\"invalid args (enc_dim={}, dec_dim={}, attn_dim={})\".format(enc_dim, dec_dim, attn_dim))\n",
    "\n",
    "    def forward(self, eh, dhx, ax=None):\n",
    "        \"\"\"\n",
    "        Forward Attention method.\n",
    "        ----\n",
    "        eh (FloatTensor): the encoder hidden state with\n",
    "            shape (batch size, time, hidden dimension).\n",
    "        dhx (FloatTensor): one time step of the decoder hidden\n",
    "            state with shape (batch size, hidden dimension).\n",
    "        ax (FloatTensor): one time step of the attention vector.\n",
    "        ----\n",
    "        Returns the context vectors (sx) and the corresponding attention alignment (ax)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.use_default:\n",
    "            # Compute inner product of decoder slice with every encoder slice\n",
    "            pax = torch.sum(eh * dhx, dim=2)\n",
    "            ax = nn.functional.softmax(pax, dim=1)\n",
    "            sx = torch.sum(eh * ax.unsqueeze(2), dim=1, keepdim=True)\n",
    "        else:\n",
    "            pass\n",
    "            # TODO: Modify the attention mechanism\n",
    "        return sx, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(results):\n",
    "    \"\"\"\n",
    "    Compute the word-error-rate (WER).\n",
    "    \"\"\"\n",
    "    dist = 0.\n",
    "    for label, pred in results:\n",
    "        dist += editdistance.eval(label, pred)\n",
    "    total = sum(len(label) for label, _ in results)\n",
    "    return dist / total\n",
    "\n",
    "def train(model, optimizer, ldr):\n",
    "    \"\"\"\n",
    "    Train the model for an epoch (one pass over the training data)\n",
    "    ----\n",
    "    model: Seq2Seq model instance\n",
    "    optimizer: torch.nn optimizer instance\n",
    "    ldr: data loader instance\n",
    "    ----\n",
    "    Returns the average loss over an epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.scheduled_sampling = model.sample_prob != 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for ii, (inputs, labels) in enumerate(ldr):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = model.collate(inputs, labels)\n",
    "        loss = model.loss(x, y)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.item())\n",
    "        \n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, ldr, preproc):\n",
    "    \"\"\"\n",
    "    Evaluate the model (on either dev or test).\n",
    "    ----\n",
    "    model: Seq2Seq model instance\n",
    "    ldr: data loader instance\n",
    "    preproc: preprocessor instance\n",
    "    ----\n",
    "    Returns the average loss and wer on a given dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.scheduled_sampling = False\n",
    "    \n",
    "    losses, hyps, refs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in ldr:\n",
    "            x, y = model.collate(inputs, labels)\n",
    "            # get loss\n",
    "            loss = model.loss(x, y)\n",
    "            losses.append(loss.data.item())\n",
    "            # get predictions\n",
    "            pred = model.infer(x, y)\n",
    "            hyps.extend(pred)\n",
    "            refs.extend(labels)\n",
    "\n",
    "    results = [(preproc.decode(r), preproc.decode(h)) for r, h in zip(refs, hyps)]\n",
    "    \n",
    "    return np.mean(losses), compute_wer(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the development set to tune your model.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"rnn/config.json\", \"r\") as fid:                                                                                                                                                                                                                                      \n",
    "    config = json.load(fid)\n",
    "\n",
    "random.seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"Training RNN\")\n",
    "data_cfg = config[\"data\"]\n",
    "model_cfg = config[\"model\"]\n",
    "opt_cfg = config[\"optimizer\"]\n",
    "\n",
    "preproc = Preprocessor(data_cfg[\"train_set\"], start_and_end=data_cfg[\"start_and_end\"])\n",
    "\n",
    "train_ldr = make_loader(data_cfg[\"train_set\"], preproc, opt_cfg[\"batch_size\"])\n",
    "dev_ldr = make_loader(data_cfg[\"dev_set\"], preproc, opt_cfg[\"batch_size\"])\n",
    "\n",
    "attention = Attention(model_cfg[\"encoder\"][\"hidden_size\"], model_cfg[\"decoder\"][\"hidden_size\"])\n",
    "model = Seq2Seq(preproc.input_dim, preproc.vocab_size, attention, model_cfg)\n",
    "model = model.cuda() if use_cuda else model.cpu()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=opt_cfg[\"learning_rate\"], momentum=opt_cfg[\"momentum\"])\n",
    "\n",
    "log=\"epoch {:4} | train_loss={:6.2f}, dev_loss={:6.2f} with {:6.2f}% WER ({:6.2f}s elapsed)\"\n",
    "\n",
    "best_so_far = float(\"inf\")\n",
    "for ep in range(opt_cfg[\"max_epochs\"]):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss = train(model, optimizer, train_ldr)    \n",
    "    dev_loss, dev_wer = evaluate(model, dev_ldr, preproc)\n",
    "    \n",
    "    print(log.format(ep + 1, train_loss, dev_loss, dev_wer * 100., time.time() - start))\n",
    "    \n",
    "    torch.save(model, os.path.join(config[\"save_path\"], str(ep)))\n",
    "    \n",
    "    if dev_wer < best_so_far:\n",
    "        best_so_far = dev_wer\n",
    "        torch.save(model, os.path.join(config[\"save_path\"], \"best\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tune on the dev set\n",
    "# may want to set up function or chunk of code here to perform tuning\n",
    "# call train on training set, call evaluate on dev, save/plot/compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RNN\")\n",
    "test_model = torch.load(os.path.join(config[\"save_path\"], \"best\"))\n",
    "test_ldr = make_loader(data_cfg[\"test_set\"], preproc, opt_cfg[\"batch_size\"])\n",
    "\n",
    "_, test_wer = evaluate(test_model, test_ldr, preproc)\n",
    "\n",
    "print(\"{:.2f}% WER (test)\".format(test_wer * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
