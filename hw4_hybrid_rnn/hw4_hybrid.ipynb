{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.seterr(divide='ignore') # masks log(0) errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid.hmm.multiple import FullGaussianHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default DNN set-up should take ~40 seconds/epoch on a GPU (and ~350 secconds/epoch on a CPU).\n",
    "\n",
    "Performance (WER) on test set:   \n",
    "\n",
    "Baseline performance of the GMM-HMM model   \n",
    "24.55%\n",
    "\n",
    "Performance of the DNN-HMM model with normalized emission probabilities   \n",
    "20.45%\n",
    "\n",
    "Performance of the DNN-HMM model with unnormalized emission probabilities   \n",
    "18.18%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a multiple digit GMM-HMM model\n",
    "NOTE: You are not expected to run/tune this part as the trained FullGaussianHMM model file is provided. The provided model is designed to have 15 states for each digit and 3 additional states for start, pause, and end. Feel free to look through hybrid/hmm/multiple.py to see how we can string single-digit HMMs to obtain the one that can model multiple-digit sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Multiple Digit HMM: training two-digit sequences\n",
    "# \"\"\"\n",
    "# data_multiple_digit = np.load(\"hybrid/data/mfccs/mfccs_multiple.npz\", allow_pickle=True)\n",
    "# full_model = FullGaussianHMM(data_multiple_digit[\"Xtrain\"], \"hybrid/hmm/models/single_digit_model.pkl\")\n",
    "\n",
    "# n_iter = 15\n",
    "\n",
    "# print(\"Training HMM\")\n",
    "# for i in range(n_iter):\n",
    "#     print(\"starting iteration {}...\".format(i + 1))\n",
    "#     full_model.train(data_multiple_digit[\"Xtrain\"], data_multiple_digit[\"Ytrain\"])\n",
    "        \n",
    "# print(\"Testing HMM\")\n",
    "# test_wer = full_model.test(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"])\n",
    "# print(\"{:.2f}% WER\".format(test_wer * 100.))\n",
    "\n",
    "# with open(\"hybrid/hmm/models/multiple_digit_model.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(full_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the optimal state sequences\n",
    "Save the optimal state label per framee using the trained GMM-HMM model. Complete the # TODO in force_align function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_align(X, Y, hmm_gmm_model):\n",
    "    \"\"\"\n",
    "    Force align using Viterbi to get the hidden state sequence for each (X, Y) pair.\n",
    "    ------\n",
    "    input:\n",
    "    X: list of 2d-arrays of shape (Tx, 13): list of single digit MFCC features\n",
    "    Y: digit sequence\n",
    "    hmm_gmm_model: load the trained model\n",
    "    ------\n",
    "    Returns a list of utterence-wise hidden state sequences\n",
    "    \"\"\"\n",
    "    digit_states_total, start_states = hmm_gmm_model.digit_states_total, hmm_gmm_model.start_states\n",
    "    begin_sil_id, pause_id, end_sil_id = hmm_gmm_model.begin_sil, hmm_gmm_model.pause, hmm_gmm_model.end_sil\n",
    "    A_estimate, pi_estimate = hmm_gmm_model.A, hmm_gmm_model.pi\n",
    "    state_seqs = []\n",
    "    for ii, (x, y) in enumerate(zip(X, Y)):\n",
    "\n",
    "        y = np.array([0 if yy == 'o' else int(yy) for yy in y], dtype=np.int32)\n",
    "\n",
    "        # TODO: edit A_estimate appropriately to enable decoding for the ground-truth labels\n",
    "        \n",
    "        log_pi = np.log(pi_estimate)\n",
    "        log_A = np.log(A_estimate)\n",
    "        log_B = hmm_gmm_model.get_emissions(x)\n",
    "\n",
    "        q, log_prob = hmm_gmm_model.viterbi(log_pi, log_A, log_B) \n",
    "        state_seqs.append(q)\n",
    "\n",
    "    return state_seqs\n",
    "\n",
    "data_multiple_digit = np.load(\"hybrid/data/mfccs/mfccs_multiple.npz\", allow_pickle=True)\n",
    "with open(\"hybrid/hmm/models/multiple_digit_model.pkl\", \"rb\") as f:\n",
    "    hmm_gmm_model = pkl.load(f)\n",
    "    \n",
    "state_seq_train = force_align(data_multiple_digit[\"Xtrain\"], data_multiple_digit[\"Ytrain\"], hmm_gmm_model)\n",
    "state_seq_dev = force_align(data_multiple_digit[\"Xdev\"], data_multiple_digit[\"Ydev\"], hmm_gmm_model)\n",
    "state_seq_test = force_align(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"], hmm_gmm_model)\n",
    "seqDict = {'Ytrain': state_seq_train, 'Ydev': state_seq_dev, 'Ytest': state_seq_test, 'total_states': hmm_gmm_model.total}\n",
    "np.savez_compressed('hybrid/data/state_seq/state_seq.npz', **seqDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DNN frame classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid.dnn.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hybrid/dnn/config.json\", \"r\") as fid:                                                                                                                                                                                                                                      \n",
    "    config = json.load(fid)\n",
    "\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "data_cfg = config[\"data\"]\n",
    "model_cfg = config[\"model\"]\n",
    "opt_cfg = config[\"optimizer\"]\n",
    "out_cfg = config[\"output\"]\n",
    "\n",
    "data_mfccs = np.load(data_cfg[\"mfcc\"], allow_pickle=True)\n",
    "state_seq = np.load(data_cfg[\"state_seq\"], allow_pickle=True)\n",
    "\n",
    "print(\"Preparing data...\\n\")\n",
    "data_ldr = DataLoader(data_cfg)\n",
    "train_features, train_labels, train_labels_onehot, train_utt_to_frames = data_ldr.prepare_data('train')\n",
    "dev_features, dev_labels, dev_labels_onehot, dev_utt_to_frames = data_ldr.prepare_data('dev')\n",
    "test_features, test_labels, test_labels_onehot, test_utt_to_frames = data_ldr.prepare_data('test')\n",
    "\n",
    "feat_dim = (data_ldr.context_size+1)*data_ldr.mfcc_dim\n",
    "n_states = data_ldr.n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim, n_states, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initialized feed forward neural network model.\n",
    "        ---\n",
    "        feat_dim: input feature dimension\n",
    "        n_states: size of the output\n",
    "        hidden_dim: dimension of the hidden layers\n",
    "        n_layers: number of layers\n",
    "        dropout: dropout probabilty for the dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.fc_input = nn.Linear(feat_dim, hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, n_states)\n",
    "        self.fc_hidden_list = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim)]*n_layers)\n",
    "        self.nl = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the feedforward network\n",
    "        \"\"\"\n",
    "        x = self.nl(self.fc_input(x))\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.nl(self.fc_hidden_list[i](x))\n",
    "        output = F.leaky_relu(self.fc_output(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Training the classifier on frame level labels\n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    perm = np.random.permutation(train_features.shape[0])\n",
    "    train_loss, pred_multi, gt_multi = [], [], []\n",
    "    n_iter = 0\n",
    "    start = time.time()\n",
    "    time_per_iter = [0]*4\n",
    "    for i in range(0, len(perm), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        train_Xs = torch.tensor(train_features[idx], dtype=torch.float32).to(device)\n",
    "        train_Ys = torch.tensor(train_labels[idx], dtype=torch.long).to(device)\n",
    "        pred_Ys = classifier(train_Xs)\n",
    "        loss = loss_function(pred_Ys, train_Ys)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classifier.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.cpu().item())\n",
    "        pred_multi.append(np.argmax(pred_Ys.cpu().data.numpy(), axis=1))\n",
    "        gt_multi.append(train_Ys.cpu().data.numpy())\n",
    "    pred_multi, gt_multi = np.concatenate(pred_multi, axis=0), np.concatenate(gt_multi, axis=0)\n",
    "    accuracy = 100*len(np.where((pred_multi - gt_multi)==0)[0])/len(pred_multi)\n",
    "    print(\"Epoch: %d, Training loss: %.2f, Accuracy: %.2f, Time elapsed: %.2f seconds\" % (epoch, np.mean(train_loss), accuracy, time.time() - start))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def test(features, labels, classifier_test=None):\n",
    "    \"\"\"\n",
    "    Training the classifier on frame level labels\n",
    "    \"\"\"\n",
    "    if classifier_test is None:\n",
    "        classifier_test = torch.load(save_model_fn)\n",
    "    classifier_test.eval()\n",
    "    test_loss, pred_multi, gt_multi = [], [], []\n",
    "    n_iter = 0\n",
    "    start = time.time()\n",
    "    for i in range(0, len(features), test_batch_size):\n",
    "        n_iter += 1\n",
    "        idx = list(range(i, min(i+test_batch_size, len(features))))\n",
    "        test_Xs = torch.tensor(features[idx], dtype=torch.float32).to(device)\n",
    "        test_Ys = torch.tensor(labels[idx], dtype=torch.long).to(device)\n",
    "        pred_Ys = classifier_test(test_Xs)\n",
    "        loss = loss_function(pred_Ys, test_Ys)\n",
    "        test_loss.append(loss.cpu().item())\n",
    "        pred_multi.append(np.argmax(pred_Ys.cpu().data.numpy(), axis=1))\n",
    "        gt_multi.append(test_Ys.cpu().data.numpy())\n",
    "\n",
    "    pred_multi, gt_multi = np.concatenate(pred_multi, axis=0), np.concatenate(gt_multi, axis=0)\n",
    "    accuracy = 100*len(np.where((pred_multi - gt_multi)==0)[0])/len(pred_multi)\n",
    "\n",
    "    print(\"Dev Accuracy: %.2f, Time elapsed: %.2f seconds\" % (accuracy, time.time() - start))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def main_train():\n",
    "    print(\"Training begins ...\\n\")\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(tot_epoch):\n",
    "        train_accuracy = train(epoch)\n",
    "        dev_accuracy = test(dev_features, dev_labels, classifier)\n",
    "        if dev_accuracy > best_accuracy:\n",
    "            best_epoch = epoch\n",
    "            torch.save(classifier, save_model_fn)\n",
    "            best_accuracy = dev_accuracy\n",
    "    print('\\nBest dev accuracy: %.2f at epoch: %d' % (best_accuracy, best_epoch))\n",
    "\n",
    "def main_test():\n",
    "    accuracy = test(test_features, test_labels)\n",
    "    print('\\nAccuracy on test set: %.2sf' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_epoch = opt_cfg[\"max_epochs\"]\n",
    "hidden_dim = model_cfg[\"hidden_dim\"]\n",
    "n_layers = model_cfg[\"n_layers\"]\n",
    "dropout = model_cfg[\"dropout_probability\"]\n",
    "\n",
    "batch_size = opt_cfg[\"batch_size\"]\n",
    "test_batch_size = opt_cfg[\"test_batch_size\"]\n",
    "\n",
    "save_model_fn = os.path.join(out_cfg[\"save_dir\"], \"dnn_model.pkl\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "classifier = FeedForward(feat_dim, n_states, hidden_dim, n_layers, dropout).to(device)\n",
    "# classifier.apply(init_weights)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = getattr(torch.optim, opt_cfg[\"type\"])(list(classifier.parameters()))\n",
    "\n",
    "main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tune on the dev set\n",
    "# may want to set up function or chunk of code here to perform tuning\n",
    "# call train on training set, call test on dev, save/plot/compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save log-emission probabilities using the best model saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_emission(utt_to_frames_dict, features, prior, temp_parameter, best_model_path):\n",
    "    \"\"\"\n",
    "    Save posteriors using the trained model\n",
    "    \"\"\"\n",
    "    classifier_eval = torch.load(best_model_path)\n",
    "    classifier_eval.eval()\n",
    "    log_emission = []\n",
    "    n_iter = 0\n",
    "    for utt_idx in range(len(utt_to_frames_dict)):\n",
    "        frame_id = utt_to_frames_dict[utt_idx]\n",
    "        log_emission_utt = []\n",
    "        for i in range(0, len(frame_id), batch_size):\n",
    "            idx = frame_id[i:i+batch_size]\n",
    "            Xs = torch.tensor(itemgetter(*idx)(features), dtype=torch.float32).to(device)\n",
    "            log_pred_Ys = F.log_softmax(classifier_eval(Xs), dim=1).cpu().data.numpy()\n",
    "            log_emission_utt.append(log_pred_Ys  - temp_parameter*np.log(prior))\n",
    "        log_emission_utt = np.concatenate(log_emission_utt, axis=0)\n",
    "        log_emission.append(log_emission_utt)\n",
    "\n",
    "    return log_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_parameter = out_cfg[\"temp_parameter\"]\n",
    "print(\"Saving log emissions for temperature %.1f ...\\n\" % (temp_parameter))\n",
    "prior = data_ldr.get_prior()\n",
    "train_log_emission =  get_log_emission(train_utt_to_frames, train_features, prior, temp_parameter, save_model_fn)\n",
    "dev_log_emission = get_log_emission(dev_utt_to_frames, dev_features, prior, temp_parameter, save_model_fn)\n",
    "test_log_emission = get_log_emission(test_utt_to_frames, test_features, prior, temp_parameter, save_model_fn)\n",
    "log_emission_dict = {'Ytrain': train_log_emission, 'Ydev': dev_log_emission, 'Ytest': test_log_emission}\n",
    "np.savez_compressed(os.path.join('hybrid/data/log_emission/log_emission_'+str(temp_parameter)+'.npz'), **log_emission_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM inference using the posterior from neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_multiple_digit = np.load(\"hybrid/data/mfccs/mfccs_multiple.npz\", allow_pickle=True)\n",
    "\n",
    "with open(\"hybrid/hmm/models/multiple_digit_model.pkl\", \"rb\") as f:\n",
    "    full_model_trained = pkl.load(f)\n",
    "\n",
    "log_emission_1 = np.load('hybrid/data/log_emission/log_emission_1.0.npz', allow_pickle=True)\n",
    "log_emission_0 = np.load('hybrid/data/log_emission/log_emission_0.0.npz', allow_pickle=True)\n",
    "    \n",
    "def get_test_wer(model, posterior=None):\n",
    "    test_wer = model.test(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"], posterior)\n",
    "    print(\"{:.2f}% TEST WER\".format(test_wer * 100.))\n",
    "\n",
    "print('Baseline performance of the trained model')\n",
    "get_test_wer(full_model_trained)\n",
    "\n",
    "print('Performance of the trained model with normalized emission probabilities')\n",
    "get_test_wer(full_model_trained, log_emission_1[\"Ytest\"])\n",
    "\n",
    "print('Performance of the trained model with unnormalized emission probabilities')\n",
    "get_test_wer(full_model_trained, log_emission_0[\"Ytest\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
