{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from glob import glob\n",
    "import soundfile as sf\n",
    "from os import path\n",
    "import numpy as np\n",
    "np.random.seed(seed=273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianHMM(object):\n",
    "    \"\"\"\n",
    "    Gaussian Hidden Markov Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, n_dims):\n",
    "        \"\"\"\n",
    "        Set up Gaussian HMM\n",
    "        ------\n",
    "        input:\n",
    "        n_states: number of states in HMM (note: one of them will be a final state)\n",
    "        n_dims: number of dimensions (13 MFCCs for this assignment)\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "    def init_gaussian_params(self, X):\n",
    "        \"\"\"\n",
    "        Initialize Gaussian parameters\n",
    "        ------\n",
    "        input:\n",
    "        X: list of 2d-arrays with shapes (Ti, 13) for example i: each is a matrix of MFCCs for a digit utterance\n",
    "        ------\n",
    "        initialize mu and sigma for each state's Gaussian (where sigma is a diagonal covariance)\n",
    "        \"\"\"\n",
    "        X_concat = np.concatenate(X)\n",
    "        self.mu = np.zeros((self.n_states, self.n_dims))\n",
    "        self.sigma = np.zeros((self.n_states, self.n_dims))\n",
    "        for s in range(self.n_states):\n",
    "            X_subset = X_concat[np.random.choice(len(X_concat), size=50, replace=False)]\n",
    "            self.mu[s] = X_subset.mean(axis=0)\n",
    "            self.sigma[s] = X_subset.var(axis=0)\n",
    "\n",
    "    def init_hmm_params(self):\n",
    "        \"\"\"\n",
    "        Initialize HMM parameters\n",
    "        ------\n",
    "        initialize pi (starting probability vector) and A (transition probabilities)\n",
    "        \"\"\"\n",
    "        self.pi = np.zeros(self.n_states)\n",
    "        self.pi[0] = 1.\n",
    "        self.A = np.zeros((self.n_states, self.n_states))\n",
    "        for s in range(self.n_states - 1):\n",
    "            self.A[s, s:s + 2] = .5\n",
    "        self.A[-1, -1] = 1.\n",
    "\n",
    "    def get_emissions(self, x):\n",
    "        \"\"\"\n",
    "        Compute Gaussian log-density at X for a diagonal model.\n",
    "        ------\n",
    "        get (continuous) emission probabilities from the multivariate normal\n",
    "        \"\"\"\n",
    "        T, _ = x.shape\n",
    "        log_B = np.zeros((self.n_states, T))\n",
    "        for s in range(self.n_states):\n",
    "            log_B[s] = multivariate_normal.logpdf(x, mean=self.mu[s], cov=np.diag(self.sigma[s]))\n",
    "        return log_B\n",
    "\n",
    "    def forward(self, log_pi, log_A_t, log_B):\n",
    "        \"\"\"\n",
    "        Forward algorithm.\n",
    "        ------\n",
    "        input:\n",
    "        log_pi: 1d-array of shape n_states: log of start probability vector\n",
    "        log_A_t: 2d-array of shape (n_states, n_states): *transposed* log of transition probability matrix\n",
    "        log_B: 2d-array of shape (n_states, Tx): log of emision probabilities (Note: Tx depends on x)\n",
    "        ------\n",
    "        output:\n",
    "        log_alpha: 2d-array of shape (n_states, Tx): log probability to state i at time t\n",
    "        \"\"\"\n",
    "        _, T = log_B.shape\n",
    "        log_alpha = np.zeros(log_B.shape)\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                log_alpha[:, t] = log_pi + log_B[:, 0]\n",
    "                #TODO: log alpha to time t\n",
    "            else:\n",
    "                log_alpha[:, t] = logsumexp(log_alpha[:, t - 1] + log_A_t, axis=1) + log_B[:, t]\n",
    "                #TODO: log alpha to time t\n",
    "        return log_alpha\n",
    "\n",
    "    def backward(self, log_A, log_B):\n",
    "        \"\"\"\n",
    "        Backward algorithm.\n",
    "        ------\n",
    "        input:\n",
    "        log_A: 2d-array of shape (n_states, n_states): log of transition probability matrix\n",
    "        log_B: 2d-array of shape (n_states, Tx): log of emision probabilities (Note: Tx depends on x)\n",
    "        ------\n",
    "        output:\n",
    "        log_beta: 2d-array of shape (n_states, Tx): log probability from state i at time t\n",
    "        \"\"\"\n",
    "        _, T = log_B.shape\n",
    "        log_beta = np.zeros(log_B.shape)\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            if t == T - 1:\n",
    "                log_beta[:, t] = 0 # log(1) = 0\n",
    "                #TODO: log beta from time t\n",
    "            else:\n",
    "                log_beta[:, t] = logsumexp(log_A + log_B[:, t + 1] + log_beta[:, t + 1], axis=1)\n",
    "                #TODO: log beta from time t\n",
    "        return log_beta\n",
    "\n",
    "    def viterbi(self, log_pi, log_A, log_B):\n",
    "        \"\"\"\n",
    "        Use viterbi algorithm to find the best path and associated log probability.\n",
    "        ------\n",
    "        input:\n",
    "        log_pi: 1d-array of shape n_states: log of start probability vector\n",
    "        log_A: 2d-array of shape (n_states, n_states): log of transition probability matrix\n",
    "        log_B: 2d-array of shape (n_states, Tx): log of emision probabilities (Note: Tx depends on x)\n",
    "        ------\n",
    "        output:\n",
    "        q: 1d-array of length T: optimal state sequence for observed sequence\n",
    "        log_prob: log probability of observed sequence\n",
    "        \"\"\"\n",
    "        _, T = log_B.shape\n",
    "        log_delta = np.zeros(log_B.shape)\n",
    "        psi = np.empty((T, self.n_states), dtype='int') # backtrace\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                log_delta[:, t] = log_pi + log_B[:, 0]\n",
    "                #TODO: find optimal state sequence\n",
    "                psi[:, t] = np.arange(self.n_states)\n",
    "            else:\n",
    "                temp = log_delta[:, t - 1] + log_A\n",
    "                log_delta[:, t] = temp.max(axis=0) + log_B[:, t]\n",
    "                #TODO: find optimal state sequence\n",
    "                psi[:, t] = temp.argmax(axis=0)\n",
    "\n",
    "        q = np.zeros(T, dtype=np.int32)\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            if t == T - 1:\n",
    "                q[t] = np.argmax(log_delta[:, -1])\n",
    "                #TODO: traceback state sequence\n",
    "                log_prob = np.max(log_delta[:, -1])\n",
    "                #TODO: log probability of observation under state sequence\n",
    "            else:\n",
    "                q[t] = psi[t, q[t + 1]]\n",
    "                #TODO: traceback state sequence\n",
    "\n",
    "        return q, log_prob\n",
    "\n",
    "    def score(self, x):\n",
    "        \"\"\"\n",
    "        Use forward-backward algorithm to\n",
    "        compute log probability and posteriors.\n",
    "        ------\n",
    "        input:\n",
    "        x :2d-array of shape (T, 13): MFCCs for a single example\n",
    "        ------\n",
    "        output:\n",
    "        log_prob :scalar: log probability of observed sequence\n",
    "        log_alpha :2d-array of shape (n_states, T): log prob of getting to state at time t from start\n",
    "        log_beta :2d-array of shape (n_states, T): log prob of getting from state at time t to end\n",
    "        gamma :2d-array of shape (n_states, T): state posterior probability\n",
    "        eps :2d-array of shape (n_states, n_states): state transition probability matrix\n",
    "        \"\"\"\n",
    "        T = len(x)\n",
    "\n",
    "        log_pi = np.log(self.pi) # starting log probabilities\n",
    "        log_A = np.log(self.A) # transition log probabilities\n",
    "        log_B = self.get_emissions(x) # emission log probabilities\n",
    "        \n",
    "        # XXX: my forward algo needs log_A.T\n",
    "        log_alpha = self.forward(log_pi, log_A.T, log_B)\n",
    "        log_beta = self.backward(log_A, log_B)\n",
    "\n",
    "        log_prob = logsumexp(log_alpha[:, -1])\n",
    "        #TODO: log probability of observations\n",
    "        debug = logsumexp(log_pi + log_B[:, 0] + log_beta[:, 0])\n",
    "        assert np.isclose(log_prob, debug)\n",
    "\n",
    "        gamma = np.exp(log_alpha + log_beta - log_prob)\n",
    "        #TODO: posteriors, no longer in log-space!\n",
    "\n",
    "        xi = np.zeros((T - 1, self.n_states, self.n_states))\n",
    "        for t in range(T - 1):\n",
    "            xi[t] = log_alpha[:, t][:, None] + log_beta[:, t + 1] + log_A + log_B[:, t + 1]\n",
    "            #TODO: transition prob i -> j for each t\n",
    "        xi -= log_prob\n",
    "        xi = np.exp(xi)\n",
    "        \n",
    "        xi = xi.sum(axis=0) # sum over time\n",
    "        xi /= xi.sum(axis=1, keepdims=True).clip(1e-1) # normalize by state probabilities (sum transitions over j)\n",
    "\n",
    "        return log_prob, log_alpha, log_beta, gamma, xi\n",
    "\n",
    "    def train(self, X):\n",
    "        \"\"\"\n",
    "        Estimate model parameters.\n",
    "        ------\n",
    "        input:\n",
    "        X: list of 2d-arrays of shape (Tx, 13): list of single digit MFCC features\n",
    "        ------\n",
    "        update model parameters (A, mu, sigma)\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"gamma\": np.zeros((self.n_states, 1)),\n",
    "            \"A\": np.zeros((self.n_states, self.n_states)),\n",
    "            \"X\": np.zeros((self.n_states, self.n_dims)),\n",
    "            \"X**2\": np.zeros((self.n_states, self.n_dims))\n",
    "        }\n",
    "\n",
    "        for x in X:\n",
    "            log_prob, log_alpha, log_beta, gamma, xi = self.score(x)\n",
    "\n",
    "            stats[\"gamma\"] += gamma.sum(axis=1, keepdims=True)\n",
    "            stats[\"A\"] += xi\n",
    "            stats[\"X\"] += gamma.dot(x)\n",
    "            stats[\"X**2\"] += gamma.dot(x**2)\n",
    "\n",
    "        stats[\"gamma\"] += 1\n",
    "        stats[\"A\"][:-1,:-1] += np.diag(np.full(self.n_states - 1, 1))\n",
    "        \n",
    "        self.mu = stats[\"X\"] / stats[\"gamma\"].sum()\n",
    "        #TODO: update means\n",
    "        self.sigma = stats[\"X**2\"] / stats[\"gamma\"].sum()\n",
    "        #TODO: update diagonal covariances\n",
    "        self.sigma = self.sigma.clip(1e-1)\n",
    "        \n",
    "        self.A = np.where(np.bitwise_or(self.A == 0.0, self.A == 1.0), self.A, stats[\"A\"]) # update transition probabilities\n",
    "        self.A /= self.A.sum(axis=1, keepdims=True) # normalize transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM for digit 0\n",
      "Starting iteration 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-5d2a8d76bd24>:160: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pi = np.log(self.pi) # starting log probabilities\n",
      "<ipython-input-8-5d2a8d76bd24>:161: RuntimeWarning: divide by zero encountered in log\n",
      "  log_A = np.log(self.A) # transition log probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1...\n",
      "Starting iteration 2...\n",
      "Starting iteration 3...\n",
      "Starting iteration 4...\n",
      "Starting iteration 5...\n",
      "Starting iteration 6...\n",
      "Starting iteration 7...\n",
      "Starting iteration 8...\n",
      "Starting iteration 9...\n",
      "Starting iteration 10...\n",
      "Starting iteration 11...\n",
      "Starting iteration 12...\n",
      "Starting iteration 13...\n",
      "Starting iteration 14...\n",
      "Training HMM for digit 1\n",
      "Starting iteration 0...\n",
      "Starting iteration 1...\n",
      "Starting iteration 2...\n",
      "Starting iteration 3...\n",
      "Starting iteration 4...\n",
      "Starting iteration 5...\n",
      "Starting iteration 6...\n",
      "Starting iteration 7...\n",
      "Starting iteration 8...\n",
      "Starting iteration 9...\n",
      "Starting iteration 10...\n",
      "Starting iteration 11...\n",
      "Starting iteration 12...\n",
      "Starting iteration 13...\n",
      "Starting iteration 14...\n",
      "Training HMM for digit 2\n",
      "Starting iteration 0...\n",
      "Starting iteration 1...\n",
      "Starting iteration 2...\n",
      "Starting iteration 3...\n",
      "Starting iteration 4...\n",
      "Starting iteration 5...\n",
      "Starting iteration 6...\n",
      "Starting iteration 7...\n",
      "Starting iteration 8...\n",
      "Starting iteration 9...\n",
      "Starting iteration 10...\n",
      "Starting iteration 11...\n",
      "Starting iteration 12...\n",
      "Starting iteration 13...\n",
      "Starting iteration 14...\n",
      "Training HMM for digit 3\n",
      "Starting iteration 0...\n",
      "Starting iteration 1...\n",
      "Starting iteration 2...\n",
      "Starting iteration 3...\n",
      "Starting iteration 4...\n",
      "Starting iteration 5...\n",
      "Starting iteration 6...\n",
      "Starting iteration 7...\n",
      "Starting iteration 8...\n",
      "Starting iteration 9...\n",
      "Starting iteration 10...\n",
      "Starting iteration 11...\n",
      "Starting iteration 12...\n",
      "Starting iteration 13...\n",
      "Starting iteration 14...\n",
      "Training HMM for digit 4\n",
      "Starting iteration 0...\n",
      "Starting iteration 1...\n",
      "Starting iteration 2...\n",
      "Starting iteration 3...\n",
      "Starting iteration 4...\n",
      "Starting iteration 5...\n",
      "Starting iteration 6...\n",
      "Starting iteration 7...\n",
      "Starting iteration 8...\n",
      "Starting iteration 9...\n",
      "Starting iteration 10...\n",
      "Starting iteration 11...\n",
      "Starting iteration 12...\n",
      "Starting iteration 13...\n",
      "Starting iteration 14...\n",
      "Training HMM for digit 5\n",
      "Starting iteration 0...\n",
      "Starting iteration 1...\n",
      "Starting iteration 2...\n",
      "Starting iteration 3...\n",
      "Starting iteration 4...\n",
      "Starting iteration 5...\n",
      "Starting iteration 6...\n",
      "Starting iteration 7...\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"mfccs.npz\", allow_pickle=True)\n",
    "Xtrain, Ytrain = dataset[\"Xtrain\"], dataset[\"Ytrain\"]\n",
    "Xtest, Ytest = dataset[\"Xtest\"], dataset[\"Ytest\"]\n",
    "\n",
    "# Expected error rates:\n",
    "# 15 states/15 iterations: 0.9714 forward, 0.9679 viterbi\n",
    "# 25 states/25 iterations: 0.9821 forward, 0.9821 viterbi\n",
    "# 50 states/50 iterations: 0.9804 forward, 0.9804 viterbi\n",
    "\n",
    "n_states = 6\n",
    "n_dims = 13\n",
    "n_iter = 15\n",
    "model = dict()\n",
    "\n",
    "digits = range(10)\n",
    "\n",
    "for digit in digits:\n",
    "    print(\"Training HMM for digit %d\" % digit)\n",
    "    Xtrain_digit = [x for x, y in zip(Xtrain, Ytrain) if y == digit]\n",
    "    model[digit] = GaussianHMM(n_states=n_states, n_dims=n_dims)\n",
    "    model[digit].init_gaussian_params(Xtrain_digit)\n",
    "    model[digit].init_hmm_params()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        print(\"Starting iteration %d...\" % i)\n",
    "        model[digit].train(Xtrain_digit)\n",
    "\n",
    "print(\"Testing HMM\")\n",
    "forward_accuracy, viterbi_accuracy = np.zeros(10), np.zeros(10)\n",
    "forward_confusion, viterbi_confusion = np.zeros((10, 10)), np.zeros((10, 10))\n",
    "for x, y in zip(Xtest, Ytest):\n",
    "    T = len(x)\n",
    "\n",
    "    forward_scores, viterbi_scores = [], []\n",
    "    for digit in digits:\n",
    "        log_pi = np.log(model[digit].pi)\n",
    "        log_A = np.log(model[digit].A)\n",
    "        log_B = model[digit].get_emissions(x)\n",
    "        \n",
    "        # XXX: my forward algo uses log_A.T\n",
    "        # might be better to run backward\n",
    "        log_alpha = model[digit].forward(log_pi, log_A.T, log_B)\n",
    "        forward_log_prob = logsumexp(log_alpha[:, T - 1])\n",
    "        _, viterbi_log_prob = model[digit].viterbi(log_pi, log_A_transposed, log_B)\n",
    "\n",
    "        forward_scores.append(forward_log_prob)\n",
    "        viterbi_scores.append(viterbi_log_prob)\n",
    "\n",
    "    forward_top_digit, forward_top_log_prob = sorted(zip(digits, forward_scores), key=lambda x: -x[1])[0]\n",
    "    viterbi_top_digit, viterbi_top_log_prob = sorted(zip(digits, viterbi_scores), key=lambda x: -x[1])[0]\n",
    "\n",
    "    forward_confusion[y, forward_top_digit] += 1.\n",
    "    viterbi_confusion[y, viterbi_top_digit] += 1.\n",
    "\n",
    "forward_accuracy = np.diag(forward_confusion) / forward_confusion.sum(axis=1)\n",
    "viterbi_accuracy = np.diag(viterbi_confusion) / viterbi_confusion.sum(axis=1)\n",
    "\n",
    "print(\"forward accuracy (%.4f)\" % forward_accuracy.mean(), forward_accuracy)\n",
    "print(\"viterbi accuracy (%.4f)\" % viterbi_accuracy.mean(), viterbi_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
